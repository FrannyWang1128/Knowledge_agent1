from __future__ import annotations as _annotations

from dataclasses import dataclass
from dotenv import load_dotenv
import logfire
import asyncio
import os
from openai import AsyncOpenAI

from pydantic_ai import Agent, RunContext
from pydantic_ai.models.openai import OpenAIModel
from supabase import Client
from typing import List
from sentence_transformers import SentenceTransformer

load_dotenv()

supabase_url = os.getenv("SUPABASE_URL1")
supabase_key = os.getenv("SUPABASE_SECRET1")
supabase = Client(supabase_url, supabase_key)

import os
load_dotenv()
print("DEBUG OPENAI_API_KEY:", os.getenv("OPENAI_API_KEY"))
print("DEBUG OPENAI_API_BASE:", os.getenv("OPENAI_API_BASE"))

openai_client = AsyncOpenAI(
    api_key  = os.getenv("OPENAI_API_KEY"),
    base_url = os.getenv("OPENAI_API_BASE"),
)

from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.openai import OpenAIModel

# Create a Provider
provider = OpenAIProvider(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE"),    # e.g. https://dashscope.aliyuncs.com/compatible-mode
)

llm_model_name = os.getenv("LLM_MODEL", "qwen-plus")
model = OpenAIModel(llm_model_name, provider=provider)


logfire.configure(send_to_logfire='if-token-present')

# Hugging face embedding model
embedding_model = SentenceTransformer("BAAI/bge-base-en-v1.5")

async def get_embedding(text: str) -> List[float]:
    try:
        return embedding_model.encode(text).tolist()
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return [0.0] * embedding_model.get_sentence_embedding_dimension()

@dataclass
class PydanticAIDeps:
    supabase: Client
    openai_client: AsyncOpenAI

system_prompt = """
You are an expert in environmental earth science data retrieval.
You MUST use the tool `retrieve_relevant_documentation_and_docs` to fetch candidate datasets before answering user queries.

You are provided with several documentation snippets about different datasets.
Your job is to answer the user's question by reviewing ALL the provided documentation below.

**Important**: You MUST include **every** dataset returned by the `retrieve_relevant_documentation_and_docs` tool in your final numbered list. Do **not** omit any item, even if it seems marginally relevant.
- Please output a numbered list, including ALL dataset names that are even partially relevant to the user's query. Each item should be: "Dataset name - one sentence description".
- If a dataset is not obviously relevant but might be useful, you should still include it.
- Do NOT invent or translate dataset names/descriptions. Only use the content given.
- If no relevant datasets are found, reply with: not found.
- DO NOT use outside knowledge or add extra analysis.

Documentation retrieved:
{context}

User question: {user_query}

Your answer (numbered list of relevant datasets):
"""

pydantic_ai_expert = Agent(
    model,
    system_prompt=system_prompt,
    deps_type=PydanticAIDeps,
    retries=2
)


@pydantic_ai_expert.tool
async def retrieve_relevant_documentation_and_docs(
    ctx: RunContext[PydanticAIDeps],
    user_query: str
) -> tuple[str, list[dict]]:
    """
    Retrieve the most semantically relevant documentation chunks using vector similarity search (embedding-based RAG).

    This function encodes the user's query as a semantic embedding and queries the Supabase backend for documentation
    chunks whose embeddings are most similar (nearest neighbors) to the query embedding, optionally applying filter conditions.
    The top results are formatted and passed to an LLM agent to generate a structured answer.

    Args:
        ctx: The context object containing Supabase and AI dependencies.
        user_query: The user's question or query (will be embedded into a vector for semantic search).

    Returns:
        A tuple containing:
            - A string answer generated by the LLM, typically a numbered list of the most relevant documentation.
            - A list of dictionaries, each with 'title' and 'url' of the matched documentation.
    """
    # 1. RAG retrival
    query_emb = await get_embedding(user_query)
    rpc_res = ctx.deps.supabase.rpc(
        'match_site_pages',
        {
            'query_embedding': query_emb,
            'match_count': 10,
            'filter': {}
        }
    ).execute()

    raw = rpc_res.data or []
    if not raw:
        docs = []
        context = "(No relevant documentation found.)"
    else:
        # 2. Assembling the docs list
        docs = [
            {"title": doc["title"], "url": doc["url"]}
            for doc in raw
        ]
        # Cache docs for show_document
        ctx.deps.last_docs = docs

        # 3. Assemble the context for LLM
        context = "\n".join(
            f"{i+1}. {d['title']} â†’ {d['url']}"
            for i, d in enumerate(docs)
        )

    # 4. Call Agent to let LLM generate answers
    prompt = f"""
Documentation retrieved:
{context}

User question: {user_query}

Your answer (numbered list of relevant datasets):
"""
    result = await pydantic_ai_expert.run(prompt, deps=ctx.deps)
    answer = result.output

    return answer, docs

@pydantic_ai_expert.tool
async def show_document(
    ctx: RunContext[PydanticAIDeps],
    index: int
) -> str:
    """
    Show the full content of a documentation chunk from the last search results by index.

    Args:
        ctx: Context with dependencies and cached search results.
        index: 0-based index of the document to display.

    Returns:
        The content of the selected document as a string, or an error message if the index is invalid.
    """
    docs = getattr(ctx.deps, "last_docs", []) or []
    if index < 0 or index >= len(docs):
        return f"Invalid index {index}, please pick between 1 and {len(docs)}."
    url = docs[index]["url"]
    return await get_page_content(ctx, url)


@pydantic_ai_expert.tool
async def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:
    """
    Retrieve a list of all available Pydantic AI documentation pages.

    Returns:
        List[str]: List of unique URLs for all documentation pages
    """
    try:
        result = ctx.deps.supabase.from_('site_pages') \
            .select('url') \
            .execute()
        if not result.data:
            return []
        urls = sorted(set(doc['url'] for doc in result.data))
        return urls
    except Exception as e:
        print(f"Error retrieving documentation pages: {e}")
        return []

@pydantic_ai_expert.tool
async def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:
    """
    Retrieve the full content of a specific documentation page by combining all its chunks.

    Args:
        ctx: The context including the Supabase client
        url: The URL of the page to retrieve

    Returns:
        str: The complete page content with all chunks combined in order
    """
    try:
        result = ctx.deps.supabase.from_('site_pages') \
            .select('title, content, chunk_number') \
            .eq('url', url) \
            .order('chunk_number') \
            .execute()
        if not result.data:
            return f"No content found for URL: {url}"
        page_title = result.data[0]['title'].split(' - ')[0]
        formatted_content = [f"# {page_title}\n"]
        for chunk in result.data:
            formatted_content.append(chunk['content'])
        return "\n\n".join(formatted_content)
    except Exception as e:
        print(f"Error retrieving page content: {e}")
        return f"Error retrieving page content: {str(e)}"


